{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Automatically reload changes to external code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "#A Convolutional Network implementation example using TensorFlow library.\n",
    "#This example is using the MNIST database of handwritten digits\n",
    "#(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "#Author: Aymeric Damien\n",
    "#Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "\n",
    "# Based on above project, modified by James Chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from mnist import loader #loader for mnist dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pdb, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Mnist dataset\n",
    "\n",
    "The MNIST database of handwritten digits. [[website]](http://yann.lecun.com/exdb/mnist/)<br>\n",
    "There are **60,000** training images and **10,000** testing images in this dataset.<br>\n",
    "Each digit is a one-channel image. Size of image = 28*28 = 784.\n",
    "\n",
    "![](imgs/mnist_ex.png)\n",
    "\n",
    "There are some build-in mnist function can be used in tensorflow.\n",
    "\n",
    "Ex.<br>\n",
    "from tensorflow.examples.tutorials.mnist import input_data<br>\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "Instead of using these functions, I'll use the orginal dataset manually in this code.<br>\n",
    "It's more clear to trace the data-processing.\n",
    "\n",
    "When we load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load mnist data manually\n",
    "# loading 'train' or 'test' data\n",
    "# ex. load_mnist_data('train')\n",
    "# return images, labels and mean of all images. (But, we'll only use the mean of training data.)\n",
    "# ims: [N * 784]\n",
    "# labels: [N]\n",
    "# ims_mean: [784]\n",
    "\n",
    "def load_mnist_data(flag, data_path='data'):\n",
    "    data_loader = loader.MNIST(data_path)\n",
    "    if flag == 'train':\n",
    "        ims, labels = data_loader.load_training()\n",
    "    elif flag == 'test':\n",
    "        ims, labels = data_loader.load_testing()\n",
    "    else:\n",
    "        raise ValueError(\"Error. Only training or testing data.\")\n",
    "    ims = ims/255.0\n",
    "    ims_mean = np.mean(ims, axis=0)\n",
    "    return ims, labels, ims_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 2\n",
    "batch_size = 100   # training batch size\n",
    "test_batch_size = 1000\n",
    "display_step = 10  # testing \n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "stddev=0.01    # standard deviation for random initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Functions of Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "\n",
    "# convolutional function\n",
    "# input: \n",
    "# x=[batch_size, height, width, channels]\n",
    "# W(Weights)=tf.Variable(shape=[kernel_size, kernel_size, input_channel, output_channel])\n",
    "# b(Biases)=tf.Variable(shape=[output_channel])\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lenet():\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(tf.float32, [None, n_input])  # mnist input images, [batch_size x 784]\n",
    "    y = tf.placeholder(tf.int32,[None])              # label, [batch_size]\n",
    "    dropout = tf.placeholder(tf.float32)  #dropout ratio\n",
    "    \n",
    "    # Declare Variables \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        'wc1': tf.Variable(tf.truncated_normal([5, 5, 1, 32], mean=0, stddev=stddev)),\n",
    "        # 5x5 conv, 32 inputs, 64 outputs\n",
    "        'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64], mean=0, stddev=stddev)),\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'wd1': tf.Variable(tf.truncated_normal([7*7*64, 1024], mean=0, stddev=stddev)),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'out': tf.Variable(tf.truncated_normal([1024, n_classes], mean=0, stddev=stddev))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'bc1': tf.Variable(tf.random_normal([32])),  # default stddev=1.0\n",
    "        'bc2': tf.Variable(tf.random_normal([64])),\n",
    "        'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    x_reshape = tf.reshape(x, shape=[-1, 28, 28, 1]) # Transfer shape. Prepare for convolution\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x_reshape, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob = 1-dropout)   # dropout ratio --> keep ratio\n",
    "\n",
    "    # Output, class prediction\n",
    "    pred = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    #one_hot_y = tf.one_hot(y, n_classes, on_value=1, off_value=0, axis=-1)\n",
    "    #one_hot_y = tf.cast(one_hot_y, tf.float32)\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=one_hot_y))\n",
    "\n",
    "    \n",
    "\n",
    "    probs = tf.nn.softmax(pred)\n",
    "    log_probs = tf.log(probs + 1e-8)\n",
    "\n",
    "    one_hot_y = tf.one_hot(y, n_classes, on_value=1, off_value=0, axis=-1)\n",
    "    #print one_hot_y.get_shape()\n",
    "    #cross_entropy_loss = - tf.mul(y,log_probs)\n",
    "    cross_entropy_loss = - tf.mul(tf.cast(one_hot_y, tf.float32),log_probs)\n",
    "    \n",
    "    loss = tf.reduce_sum(cross_entropy_loss)\n",
    "\n",
    "\n",
    "    return x, y, dropout, loss, pred, one_hot_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter):\n",
    "    Train_Loss = 0\n",
    "    Test_Loss = 0\n",
    "    Train_Acc = 0\n",
    "    Test_Acc = 0\n",
    "    for idx in xrange(iter_per_epoch):\n",
    "        batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "        batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.0})\n",
    "        Train_Loss += C/batch_size   # calculate the loss in average (per image).\n",
    "        Train_Acc += A\n",
    "    # Eval testing dataset\n",
    "    for idx in xrange(test_iter):\n",
    "        batch_xs = ims_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]] - ims_mean\n",
    "        batch_ys = labels_test[order_list[idx*test_batch_size:(idx+1)*test_batch_size]]\n",
    "        C, A = sess.run([cost, accuracy], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.0})\n",
    "        Test_Loss += C/test_batch_size\n",
    "        Test_Acc += A\n",
    "    return Train_Loss, Train_Acc, Test_Loss, Test_Acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Main function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------After Random Initialization------\n",
      "Training: loss=3.129127, acc=0.097367.\t\tTesting: loss=3.126546, acc=0.098200\n",
      " 1.871381 seconds\n",
      "------Start Training------\n",
      "Epoch 0.000000, Training: loss=2.475106, acc=0.098717.\t\tTesting: loss=2.475202, acc=0.098000\n",
      "Epoch 0.016667, Training: loss=2.321483, acc=0.104417.\t\tTesting: loss=2.321482, acc=0.102800\n",
      "Epoch 0.033333, Training: loss=2.316925, acc=0.098633.\t\tTesting: loss=2.316992, acc=0.095800\n",
      "Epoch 0.050000, Training: loss=2.312599, acc=0.112367.\t\tTesting: loss=2.312342, acc=0.113500\n",
      "Epoch 0.066667, Training: loss=2.312945, acc=0.099150.\t\tTesting: loss=2.312562, acc=0.100900\n",
      "Epoch 0.083333, Training: loss=2.307034, acc=0.112367.\t\tTesting: loss=2.307620, acc=0.113500\n",
      "Epoch 0.100000, Training: loss=2.281481, acc=0.126617.\t\tTesting: loss=2.281144, acc=0.130500\n",
      "Epoch 0.116667, Training: loss=2.196689, acc=0.273450.\t\tTesting: loss=2.195588, acc=0.278400\n",
      "Epoch 0.133333, Training: loss=2.035428, acc=0.262650.\t\tTesting: loss=2.027315, acc=0.264700\n",
      "Epoch 0.150000, Training: loss=1.815727, acc=0.489183.\t\tTesting: loss=1.814688, acc=0.501400\n",
      "Epoch 0.166667, Training: loss=1.369758, acc=0.607450.\t\tTesting: loss=1.356181, acc=0.620500\n",
      "Epoch 0.183333, Training: loss=1.059480, acc=0.678833.\t\tTesting: loss=1.044395, acc=0.685800\n",
      "Epoch 0.200000, Training: loss=0.916225, acc=0.705550.\t\tTesting: loss=0.901650, acc=0.710600\n",
      "Epoch 0.216667, Training: loss=0.841073, acc=0.741300.\t\tTesting: loss=0.815823, acc=0.751300\n",
      "Epoch 0.233333, Training: loss=0.760666, acc=0.750333.\t\tTesting: loss=0.739631, acc=0.762200\n",
      "Epoch 0.250000, Training: loss=0.660504, acc=0.793900.\t\tTesting: loss=0.633300, acc=0.811400\n",
      "Epoch 0.266667, Training: loss=0.576136, acc=0.826733.\t\tTesting: loss=0.551414, acc=0.840100\n",
      "Epoch 0.283333, Training: loss=0.554463, acc=0.824933.\t\tTesting: loss=0.531875, acc=0.835800\n",
      "Epoch 0.300000, Training: loss=0.516938, acc=0.835667.\t\tTesting: loss=0.496081, acc=0.842100\n",
      "Epoch 0.316667, Training: loss=0.462390, acc=0.854317.\t\tTesting: loss=0.445125, acc=0.860700\n",
      "Epoch 0.333333, Training: loss=0.454441, acc=0.857233.\t\tTesting: loss=0.428337, acc=0.867600\n",
      "Epoch 0.350000, Training: loss=0.412987, acc=0.867033.\t\tTesting: loss=0.399153, acc=0.872000\n",
      "Epoch 0.366667, Training: loss=0.403084, acc=0.868517.\t\tTesting: loss=0.378575, acc=0.876200\n",
      "Epoch 0.383333, Training: loss=0.383124, acc=0.875300.\t\tTesting: loss=0.367209, acc=0.880600\n",
      "Epoch 0.400000, Training: loss=0.361464, acc=0.886050.\t\tTesting: loss=0.338146, acc=0.893200\n",
      "Epoch 0.416667, Training: loss=0.325592, acc=0.897583.\t\tTesting: loss=0.315446, acc=0.900200\n",
      "Epoch 0.433333, Training: loss=0.320066, acc=0.900467.\t\tTesting: loss=0.310331, acc=0.905200\n",
      "Epoch 0.450000, Training: loss=0.309135, acc=0.904850.\t\tTesting: loss=0.295070, acc=0.909600\n",
      "Epoch 0.466667, Training: loss=0.283950, acc=0.912083.\t\tTesting: loss=0.267624, acc=0.914600\n",
      "Epoch 0.483333, Training: loss=0.283484, acc=0.911233.\t\tTesting: loss=0.265525, acc=0.917800\n",
      "Epoch 0.500000, Training: loss=0.292837, acc=0.909967.\t\tTesting: loss=0.278201, acc=0.911600\n",
      "Epoch 0.516667, Training: loss=0.291948, acc=0.909450.\t\tTesting: loss=0.278940, acc=0.914000\n",
      "Epoch 0.533333, Training: loss=0.271986, acc=0.912850.\t\tTesting: loss=0.251838, acc=0.919200\n",
      "Epoch 0.550000, Training: loss=0.273245, acc=0.912333.\t\tTesting: loss=0.257604, acc=0.916400\n",
      "Epoch 0.566667, Training: loss=0.258514, acc=0.917483.\t\tTesting: loss=0.241945, acc=0.921100\n",
      "Epoch 0.583333, Training: loss=0.225681, acc=0.931033.\t\tTesting: loss=0.213165, acc=0.935300\n",
      "Epoch 0.600000, Training: loss=0.219601, acc=0.930817.\t\tTesting: loss=0.208597, acc=0.931200\n",
      "Epoch 0.616667, Training: loss=0.259401, acc=0.917867.\t\tTesting: loss=0.244603, acc=0.918600\n",
      "Epoch 0.633333, Training: loss=0.232219, acc=0.928317.\t\tTesting: loss=0.214855, acc=0.931200\n",
      "Epoch 0.650000, Training: loss=0.239826, acc=0.925400.\t\tTesting: loss=0.231093, acc=0.926500\n",
      "Epoch 0.666667, Training: loss=0.214261, acc=0.932733.\t\tTesting: loss=0.208191, acc=0.935800\n",
      "Epoch 0.683333, Training: loss=0.224379, acc=0.929350.\t\tTesting: loss=0.211344, acc=0.932200\n",
      "Epoch 0.700000, Training: loss=0.213760, acc=0.933350.\t\tTesting: loss=0.205428, acc=0.932800\n",
      "Epoch 0.716667, Training: loss=0.202308, acc=0.937700.\t\tTesting: loss=0.193191, acc=0.940400\n",
      "Epoch 0.733333, Training: loss=0.192413, acc=0.938317.\t\tTesting: loss=0.184251, acc=0.940700\n",
      "Epoch 0.750000, Training: loss=0.195157, acc=0.938750.\t\tTesting: loss=0.188768, acc=0.941800\n",
      "Epoch 0.766667, Training: loss=0.197476, acc=0.936717.\t\tTesting: loss=0.188110, acc=0.939600\n",
      "Epoch 0.783333, Training: loss=0.186554, acc=0.941033.\t\tTesting: loss=0.179528, acc=0.941400\n",
      "Epoch 0.800000, Training: loss=0.179053, acc=0.945650.\t\tTesting: loss=0.172040, acc=0.949000\n",
      "Epoch 0.816667, Training: loss=0.182140, acc=0.942717.\t\tTesting: loss=0.170875, acc=0.944800\n",
      "Epoch 0.833333, Training: loss=0.178663, acc=0.944100.\t\tTesting: loss=0.168366, acc=0.947600\n",
      "Epoch 0.850000, Training: loss=0.197387, acc=0.936517.\t\tTesting: loss=0.186468, acc=0.940700\n",
      "Epoch 0.866667, Training: loss=0.179417, acc=0.946433.\t\tTesting: loss=0.169253, acc=0.947200\n",
      "Epoch 0.883333, Training: loss=0.167618, acc=0.947033.\t\tTesting: loss=0.155959, acc=0.949700\n",
      "Epoch 0.900000, Training: loss=0.169335, acc=0.947333.\t\tTesting: loss=0.161836, acc=0.946600\n",
      "Epoch 0.916667, Training: loss=0.169923, acc=0.947150.\t\tTesting: loss=0.161773, acc=0.948400\n",
      "Epoch 0.933333, Training: loss=0.209848, acc=0.933267.\t\tTesting: loss=0.195387, acc=0.935300\n",
      "Epoch 0.950000, Training: loss=0.170431, acc=0.946417.\t\tTesting: loss=0.162876, acc=0.945900\n",
      "Epoch 0.966667, Training: loss=0.166858, acc=0.946333.\t\tTesting: loss=0.160882, acc=0.949100\n",
      "Epoch 0.983333, Training: loss=0.159704, acc=0.948000.\t\tTesting: loss=0.153965, acc=0.946700\n",
      "Epoch 1, Training: loss=0.176514, acc=0.942083.\t\tTesting: loss=0.169427, acc=0.943700\n",
      "Cost 112.830350 seconds\n",
      "Epoch 1.000000, Training: loss=0.177840, acc=0.941467.\t\tTesting: loss=0.171101, acc=0.943000\n",
      "Epoch 1.016667, Training: loss=0.152135, acc=0.951517.\t\tTesting: loss=0.147585, acc=0.952700\n",
      "Epoch 1.033333, Training: loss=0.158997, acc=0.948933.\t\tTesting: loss=0.151962, acc=0.951600\n",
      "Epoch 1.050000, Training: loss=0.152471, acc=0.951550.\t\tTesting: loss=0.148181, acc=0.954200\n",
      "Epoch 1.066667, Training: loss=0.151193, acc=0.951800.\t\tTesting: loss=0.146477, acc=0.950100\n",
      "Epoch 1.083333, Training: loss=0.143933, acc=0.954833.\t\tTesting: loss=0.139990, acc=0.956500\n",
      "Epoch 1.100000, Training: loss=0.146069, acc=0.953683.\t\tTesting: loss=0.139804, acc=0.955400\n",
      "Epoch 1.116667, Training: loss=0.167323, acc=0.946133.\t\tTesting: loss=0.158194, acc=0.949400\n",
      "Epoch 1.133333, Training: loss=0.147538, acc=0.953417.\t\tTesting: loss=0.143283, acc=0.953200\n",
      "Epoch 1.150000, Training: loss=0.137754, acc=0.957383.\t\tTesting: loss=0.131906, acc=0.958100\n",
      "Epoch 1.166667, Training: loss=0.147603, acc=0.953750.\t\tTesting: loss=0.144795, acc=0.952500\n",
      "Epoch 1.183333, Training: loss=0.178888, acc=0.941833.\t\tTesting: loss=0.171399, acc=0.942900\n",
      "Epoch 1.200000, Training: loss=0.175931, acc=0.944550.\t\tTesting: loss=0.176873, acc=0.944800\n",
      "Epoch 1.216667, Training: loss=0.155498, acc=0.950217.\t\tTesting: loss=0.149213, acc=0.949100\n",
      "Epoch 1.233333, Training: loss=0.135276, acc=0.957833.\t\tTesting: loss=0.128313, acc=0.958100\n",
      "Epoch 1.250000, Training: loss=0.133203, acc=0.958233.\t\tTesting: loss=0.125500, acc=0.959700\n",
      "Epoch 1.266667, Training: loss=0.140586, acc=0.955783.\t\tTesting: loss=0.132046, acc=0.955300\n",
      "Epoch 1.283333, Training: loss=0.135644, acc=0.956750.\t\tTesting: loss=0.126590, acc=0.959500\n",
      "Epoch 1.300000, Training: loss=0.137844, acc=0.955700.\t\tTesting: loss=0.128983, acc=0.954800\n",
      "Epoch 1.316667, Training: loss=0.131875, acc=0.956683.\t\tTesting: loss=0.125383, acc=0.958300\n",
      "Epoch 1.333333, Training: loss=0.129758, acc=0.958900.\t\tTesting: loss=0.125312, acc=0.957500\n",
      "Epoch 1.350000, Training: loss=0.138607, acc=0.956133.\t\tTesting: loss=0.128879, acc=0.955200\n",
      "Epoch 1.366667, Training: loss=0.119131, acc=0.962217.\t\tTesting: loss=0.112128, acc=0.961300\n",
      "Epoch 1.383333, Training: loss=0.126097, acc=0.959617.\t\tTesting: loss=0.117540, acc=0.961600\n",
      "Epoch 1.400000, Training: loss=0.115345, acc=0.963450.\t\tTesting: loss=0.107832, acc=0.964100\n",
      "Epoch 1.416667, Training: loss=0.137763, acc=0.955400.\t\tTesting: loss=0.128216, acc=0.957600\n",
      "Epoch 1.433333, Training: loss=0.122596, acc=0.961450.\t\tTesting: loss=0.115837, acc=0.960500\n",
      "Epoch 1.450000, Training: loss=0.128905, acc=0.958350.\t\tTesting: loss=0.121481, acc=0.960800\n",
      "Epoch 1.466667, Training: loss=0.121330, acc=0.961917.\t\tTesting: loss=0.113474, acc=0.962000\n",
      "Epoch 1.483333, Training: loss=0.118601, acc=0.962417.\t\tTesting: loss=0.111732, acc=0.962900\n",
      "Epoch 1.500000, Training: loss=0.114894, acc=0.963817.\t\tTesting: loss=0.110668, acc=0.963400\n",
      "Epoch 1.516667, Training: loss=0.116174, acc=0.962933.\t\tTesting: loss=0.109801, acc=0.964600\n",
      "Epoch 1.533333, Training: loss=0.107864, acc=0.965450.\t\tTesting: loss=0.102408, acc=0.965200\n",
      "Epoch 1.550000, Training: loss=0.130202, acc=0.958233.\t\tTesting: loss=0.119585, acc=0.960200\n",
      "Epoch 1.566667, Training: loss=0.115548, acc=0.963450.\t\tTesting: loss=0.107888, acc=0.964100\n",
      "Epoch 1.583333, Training: loss=0.114361, acc=0.962983.\t\tTesting: loss=0.113616, acc=0.962000\n",
      "Epoch 1.600000, Training: loss=0.111242, acc=0.965183.\t\tTesting: loss=0.108987, acc=0.964700\n",
      "Epoch 1.616667, Training: loss=0.111875, acc=0.963400.\t\tTesting: loss=0.104606, acc=0.964800\n",
      "Epoch 1.633333, Training: loss=0.119034, acc=0.963100.\t\tTesting: loss=0.112345, acc=0.965300\n",
      "Epoch 1.650000, Training: loss=0.115571, acc=0.963733.\t\tTesting: loss=0.111207, acc=0.962700\n",
      "Epoch 1.666667, Training: loss=0.119104, acc=0.962533.\t\tTesting: loss=0.121019, acc=0.960100\n",
      "Epoch 1.683333, Training: loss=0.117201, acc=0.962800.\t\tTesting: loss=0.112557, acc=0.962400\n",
      "Epoch 1.700000, Training: loss=0.104894, acc=0.966583.\t\tTesting: loss=0.101680, acc=0.965700\n",
      "Epoch 1.716667, Training: loss=0.104285, acc=0.967483.\t\tTesting: loss=0.102085, acc=0.968600\n",
      "Epoch 1.733333, Training: loss=0.103211, acc=0.966350.\t\tTesting: loss=0.101684, acc=0.965800\n",
      "Epoch 1.750000, Training: loss=0.107196, acc=0.966300.\t\tTesting: loss=0.103207, acc=0.966300\n",
      "Epoch 1.766667, Training: loss=0.102150, acc=0.967633.\t\tTesting: loss=0.103899, acc=0.965700\n",
      "Epoch 1.783333, Training: loss=0.106588, acc=0.965733.\t\tTesting: loss=0.106155, acc=0.965600\n",
      "Epoch 1.800000, Training: loss=0.108727, acc=0.965900.\t\tTesting: loss=0.112220, acc=0.963900\n",
      "Epoch 1.816667, Training: loss=0.110446, acc=0.963917.\t\tTesting: loss=0.104618, acc=0.966700\n",
      "Epoch 1.833333, Training: loss=0.101863, acc=0.968467.\t\tTesting: loss=0.097262, acc=0.967000\n",
      "Epoch 1.850000, Training: loss=0.110958, acc=0.964750.\t\tTesting: loss=0.105595, acc=0.965000\n",
      "Epoch 1.866667, Training: loss=0.106123, acc=0.967183.\t\tTesting: loss=0.104649, acc=0.966600\n",
      "Epoch 1.883333, Training: loss=0.097190, acc=0.969683.\t\tTesting: loss=0.095853, acc=0.968300\n",
      "Epoch 1.900000, Training: loss=0.099221, acc=0.969467.\t\tTesting: loss=0.099109, acc=0.968700\n",
      "Epoch 1.916667, Training: loss=0.101561, acc=0.967717.\t\tTesting: loss=0.101232, acc=0.965900\n",
      "Epoch 1.933333, Training: loss=0.100365, acc=0.969250.\t\tTesting: loss=0.098178, acc=0.967300\n",
      "Epoch 1.950000, Training: loss=0.097274, acc=0.969550.\t\tTesting: loss=0.096598, acc=0.966800\n",
      "Epoch 1.966667, Training: loss=0.100346, acc=0.968550.\t\tTesting: loss=0.102999, acc=0.966700\n",
      "Epoch 1.983333, Training: loss=0.096744, acc=0.968900.\t\tTesting: loss=0.099960, acc=0.966500\n",
      "Epoch 2, Training: loss=0.111747, acc=0.963517.\t\tTesting: loss=0.109319, acc=0.963200\n",
      "Cost 111.096808 seconds\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# loading training and testing data\n",
    "ims, labels, ims_mean = load_mnist_data('train', data_path='data')\n",
    "ims_test, labels_test, _ = load_mnist_data('test', data_path='data')\n",
    "\n",
    "order_list = range(len(ims))\n",
    "\n",
    "# parameters related to mnist dataset \n",
    "test_iter = len(ims_test)/test_batch_size # number of testing-minibatch.\n",
    "\n",
    "iter_per_epoch = len(ims)/batch_size      # number of training-minibatch.\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    epoch = 0\n",
    "    \n",
    "    step = 0\n",
    "    # Keep training until reach max iterations\n",
    "    x, y, dropout, cost, pred, one_hot_y = lenet()\n",
    "    train_loss = cost/batch_size # loss per image\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "\n",
    "\n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(one_hot_y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # initialize all variables\n",
    "    try:\n",
    "        init = tf.initialize_all_variables()\n",
    "    except:\n",
    "        init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Before Training (Random initialization), Evaluate the model one-time.\n",
    "    begin = time.time()\n",
    "    Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "    print \"------After Random Initialization------\"\n",
    "    print \"Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\" %(Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch,\n",
    "                                                                     Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "    duration = time.time()-begin\n",
    "    print \" %f seconds\"%(duration)\n",
    "    \n",
    "    print \"------Start Training------\"\n",
    "    for epoch in xrange(training_epochs):\n",
    "        begin = time.time()\n",
    "        Train_Loss = 0\n",
    "        Test_Loss = 0\n",
    "        Train_Acc = 0\n",
    "        Test_Acc = 0\n",
    "        for idx in xrange(iter_per_epoch):\n",
    "            batch_xs = ims[order_list[idx*batch_size:(idx+1)*batch_size]] - ims_mean\n",
    "            batch_ys = labels[order_list[idx*batch_size:(idx+1)*batch_size]]\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run([optimizer], feed_dict={x: batch_xs, y: batch_ys, dropout: 0.5})\n",
    "            if step % display_step == 0:\n",
    "                Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "                print \"Epoch %f, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(float(step)/iter_per_epoch, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "            step += 1\n",
    "        \n",
    "        # Evaluate after each epoch finished.\n",
    "        Train_Loss, Train_Acc, Test_Loss, Test_Acc=eval_model(sess, x, y, dropout, ims, labels, ims_test, labels_test, ims_mean, iter_per_epoch, test_iter)\n",
    "        print \"Epoch %d, Training: loss=%f, acc=%f.\\t\\tTesting: loss=%f, acc=%f\"%(epoch+1, Train_Loss/iter_per_epoch, Train_Acc/iter_per_epoch, \n",
    "                                                                                         Test_Loss/test_iter, Test_Acc/test_iter)\n",
    "        duration = time.time()-begin\n",
    "        print \"Cost %f seconds\"%(duration)\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tensorflow python API\n",
    "\n",
    "### tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "\n",
    "Computes a 2-D convolution given 4-D input and filter tensors.\n",
    "\n",
    "Given an **input tensor of shape [batch, in_height, in_width, in_channels]** and a **filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]**, this op performs the following:\n",
    "\n",
    "Flattens the filter to a 2-D matrix with shape [filter_height * filter_width * in_channels, output_channels].\n",
    "\n",
    "Extracts image patches from the input tensor to form a virtual tensor of shape [batch, out_height, out_width, filter_height * filter_width * in_channels].\n",
    "For each patch, right-multiplies the filter matrix and the image patch vector.\n",
    "\n",
    "In detail, with the default NHWC format,\n",
    "\n",
    "output[b, i, j, k] = sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] * filter[di, dj, q, k]\n",
    "\n",
    "\n",
    "Must have strides[0] = strides[3] = 1. For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1].\n",
    "\n",
    "**Args:**\n",
    "\n",
    "> input: A Tensor. Must be one of the following types: half, float32, float64.\n",
    "\n",
    "> filter: A Tensor. Must have the same type as input.\n",
    "\n",
    "> strides: A list of ints. 1-D of length 4. The stride of the sliding window for each dimension of input. Must be in the same order as the dimension specified with format.\n",
    "\n",
    "> padding: A string from: \"SAME\", \"VALID\". The type of padding algorithm to use.\n",
    "use_cudnn_on_gpu: An optional bool. Defaults to True.\n",
    "\n",
    "> data_format: An optional string from: \"NHWC\", \"NCHW\". Defaults to \"NHWC\". Specify the data format of the input and output data. With the default format \"NHWC\", the data is stored in the order of: [batch, in_height, in_width,\n",
    "in_channels]. Alternatively, the format could be \"NCHW\", the data storage order of: [batch, in_channels, in_height, in_width].\n",
    "\n",
    "> name: A name for the operation (optional).\n",
    "\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "> A Tensor. Has the same type as input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
